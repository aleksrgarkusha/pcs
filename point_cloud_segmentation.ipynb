{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import pypcs\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from utils import download_pix4d_dataset, load_point_cloud, generate_features, estimate_ious\n"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "USE_COLORS = True # if False only geometric features will be used\n",
    "DATASET_PATH = \"datasets\" # Name of folder for datasets (relative to the root of the repository)\n",
    "\n",
    "# Parameters for FeatureEstimator class (set according to the paper)\n",
    "feature_estimator_kwargs = {\n",
    "    \"voxel_size\": 0.05, # size of voxel for first level of scale pyramid\n",
    "    \"num_neighbors\": 10, # number of neighbors used for point features estimation\n",
    "    \"num_scales\": 9, # number of downsampling levels\n",
    "    \"batch_size\": 10000, # number of points to process in parallel in the case of batched iteration\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Download dataset and unpack it (internet connection required)\n",
    "# You can download it manually from pix4d site and unpack in DATASET_PATH folder\n",
    "download_pix4d_dataset(DATASET_PATH)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading dataset. It will take a while\n",
      "Unpacking dataset\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Load point cloud with labels and prepare features\n",
    "train_filename = os.path.join(DATASET_PATH, \"cadastre.xyz\")\n",
    "point_cloud, labels = load_point_cloud(train_filename, USE_COLORS)\n",
    "feature_estimator = pypcs.FeatureEstimator(point_cloud, **feature_estimator_kwargs)\n",
    "features = generate_features(feature_estimator)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading cadastre.xyz: 100%|██████████| 5771358/5771358 [00:57<00:00, 100794.97it/s]\n",
      "Calculate point features: 100%|██████████| 578/578 [05:31<00:00,  1.74it/s]\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Parameters for gradient boosting (set according to the paper)\n",
    "lgbm_kwargs = {\n",
    "    \"num_leaves\": 16,\n",
    "    \"learning_rate\": 0.2,\n",
    "}\n",
    "\n",
    "# Train gradient boosting classifier\n",
    "estimator = lgb.LGBMClassifier(**lgbm_kwargs).fit(features, labels)\n"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Delete unused variables to free some RAM\n",
    "del features, labels\n",
    "gc.collect()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Load test dataset and use trained classifier to obtain semantic segmentation results\n",
    "test_filename = os.path.join(DATASET_PATH, \"ankeny.xyz\")\n",
    "point_cloud, labels = load_point_cloud(test_filename, USE_COLORS)\n",
    "feature_estimator = pypcs.FeatureEstimator(point_cloud, **feature_estimator_kwargs)\n",
    "features = generate_features(feature_estimator)\n",
    "predicted_probs = estimator.predict_proba(features)\n",
    "predicted_indices = np.argmax(predicted_probs, axis=1)\n",
    "predicted_labels = [estimator.classes_[x] for x in predicted_indices]\n",
    "\n",
    "# Calculate metrics\n",
    "print('Accuracy = {:.4%}'.format(accuracy_score(labels, predicted_labels)))\n",
    "mean_iou, ious = estimate_ious(labels, predicted_labels, estimator.n_classes_)\n",
    "print('Mean IoU = {:.4%}'.format(mean_iou))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading ankeny.xyz: 100%|██████████| 8924118/8924118 [01:24<00:00, 105518.04it/s]\n",
      "Calculate point features: 100%|██████████| 893/893 [07:59<00:00,  1.86it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy = 83.4697%\n",
      "Mean IoU = 47.2275%\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Since every point classified independently, results contain some amount of \"salt and pepper\" noise\n",
    "# we can supress this noise by averaging class labels with respect to the neighboring points classes,\n",
    "# either with majority voting scheme (this method can be used directly with class labels) ...\n",
    "predicted_labels = feature_estimator.hard_voting_smoothing(predicted_labels, num_neighbors = 20)\n",
    "print('Accuracy = {:.4%}'.format(accuracy_score(labels, predicted_labels)))\n",
    "mean_iou, ious = estimate_ious(labels, predicted_labels, estimator.n_classes_)\n",
    "print('Mean IoU = {:.4%}'.format(mean_iou))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy = 84.4055%\n",
      "Mean IoU = 48.7797%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# ... either with soft voting scheme \n",
    "# (note that this method requre class probabilities instead of class labels, but can give slightly better results)\n",
    "predicted_indices = feature_estimator.soft_voting_smoothing(predicted_probs, num_neighbors = 20)\n",
    "predicted_labels = [estimator.classes_[x] for x in predicted_indices]\n",
    "print('Accuracy = {:.4%}'.format(accuracy_score(labels, predicted_labels)))\n",
    "mean_iou, ious = estimate_ious(labels, predicted_labels, estimator.n_classes_)\n",
    "print('Mean IoU = {:.4%}'.format(mean_iou))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy = 84.6450%\n",
      "Mean IoU = 49.0840%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Now we can write semantic segmentation results to ply file (can be opened with Meshlab)...\n",
    "from utils import write_ply\n",
    "color_maapping = {\n",
    "    2 : [245, 14, 14],\n",
    "    5 : [14, 245, 29],\n",
    "    6 : [6, 57, 223],\n",
    "    11 : [6, 6, 6],\n",
    "    66 : [245, 252, 60],\n",
    "    67 : [9, 243, 243],\n",
    "}\n",
    "points = point_cloud.get_points()\n",
    "points = points - points.min(axis=0) # subtract min value to avoid underflow of float32 values\n",
    "colors = np.array([color_maapping[l] for l in predicted_labels], dtype=np.uint8)\n",
    "write_ply('predicted.ply', [points, colors], [\"x\", \"y\", \"z\", \"red\", \"green\", \"blue\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# ... or visualize directly in the browser\n",
    "import pyvista as pv\n",
    "\n",
    "visualize_every = 10 # browser visualization can be difficult, so subsample point cloud\n",
    "points = point_cloud.get_points()[::visualize_every, :]\n",
    "points = points - points.min(axis=0) # subtract min value to avoid underflow of float32 values\n",
    "viz_cloud = pv.PolyData(points.astype(np.float32))\n",
    "viz_cloud['labels'] = np.array(predicted_indices, dtype=np.uint32)[::visualize_every]\n",
    "viz_cloud.plot(cmap='gist_rainbow')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-10 00:59:32.102 Python[48131:5763619] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/b0/c1kpy9ld6g923hxpjt8jqmqm0000gn/T/org.python.python.savedState\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "ViewInteractiveWidget(height=768, layout=Layout(height='auto', width='100%'), width=1024)"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "379f2771eff8419bbbb167f88710ff43"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.4 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}